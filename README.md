This is self-practice for building LLM from scratch. I used Burmese Dataset collected from WIKI articles.
First stage is building smiple BigramLanguageModel without self attention mechanism

NOTE: The words may not be meaningful coz the bigram model will only look at the previous single token or word to generate new token


VERSION 1
_________________________________________________________________________

Loss

![Screenshot from 2024-02-01 14-37-29](https://github.com/myominhtet/BurmeseGPT-Character_Level-/assets/30900212/acbe4131-f7d3-4f4d-967b-b83f80d19a25)

Generation samples

![Screenshot from 2024-02-01 14-11-02](https://github.com/myominhtet/BurmeseGPT-Character_Level-/assets/30900212/1e007bf1-ce00-4e50-8e78-213bdcf21f45)

_________________________________________________________________________

Dataset

https://www.kaggle.com/datasets/myominhtet/burmese-wikipedia-articles144k
